{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>cuisine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Vietnamese Pho Soup</td>\n",
       "      <td>['8 cups beef broth', '4 cups water', '1 yello...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Vietnamese Chicken Meatballs</td>\n",
       "      <td>['1 pound ground chicken', '3 tablespoons fish...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Vietnamese Restaurant-Style Grilled Lemongrass...</td>\n",
       "      <td>['1 pound pork blade steaks (boneless, about 1...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Vietnamese Caramel Chicken</td>\n",
       "      <td>['1 pound chicken thighs (with skin, deboned)'...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>Cheater Vietnamese Pho (Pho Bo)</td>\n",
       "      <td>['3 marrow bones (beef bone)', '50 ounces beef...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Authentic Vietnamese Beef Pho (Pho Bo)</td>\n",
       "      <td>['3 pounds beef (knuckles or neck bones with m...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Vietnamese Pho Recipe (Vietnamese Beef Noodle ...</td>\n",
       "      <td>['600 grams onions (about 3 to 4 medium sized ...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>Pho {Vietnamese Beef Noodle Soup}</td>\n",
       "      <td>['2 tablespoons coriander seeds (whole)', '1 c...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>Vietnamese Spring Rolls</td>\n",
       "      <td>['12 spring rolls (wrappers)', '18 cooked shri...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>Fresh and Easy Vietnamese Noodle Salad</td>\n",
       "      <td>['12 ounces vermicelli noodles (thin Asian, su...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>Vietnamese Lemongrass Chicken</td>\n",
       "      <td>['4 chicken thighs (boneless, skin on, Approxi...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>Easy Vietnamese Spring Roll</td>\n",
       "      <td>['3/4 cup rice noodles (Vermicelli)', '1 packa...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>Pho (Vietnamese Noodle Soup)</td>\n",
       "      <td>['1 white onion (large, peeled and halved)', '...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>Vietnamese Pho Soup</td>\n",
       "      <td>['185 grams soup paste (jar AYAM™ Vietnamese P...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>Shrimp Pho - Vietnamese Noodle Soup</td>\n",
       "      <td>['8 ounces rice noodles (thin or pad thai styl...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>Grilled Vietnamese Chicken</td>\n",
       "      <td>['1 pound boneless skinless chicken thighs (or...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>Vietnamese Fresh Spring Rolls</td>\n",
       "      <td>['2 ounces rice noodles (rice sticks)', '1/4 p...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>Vietnamese Pho</td>\n",
       "      <td>['2 cups beef stock', '1 cinnamon (quill)', '2...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>Vietnamese Zoodle Pho</td>\n",
       "      <td>['1 sweet onion (large)', '2 inches fresh ging...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>Vietnamese Pickles</td>\n",
       "      <td>['1 pound carrots (about 3 medium carrots, pee...</td>\n",
       "      <td>Vietnamese</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  index                                               name  \\\n",
       "0            1      1                                Vietnamese Pho Soup   \n",
       "1            2      2                       Vietnamese Chicken Meatballs   \n",
       "2            3      3  Vietnamese Restaurant-Style Grilled Lemongrass...   \n",
       "3            4      4                         Vietnamese Caramel Chicken   \n",
       "4            5      5                    Cheater Vietnamese Pho (Pho Bo)   \n",
       "5            6      6             Authentic Vietnamese Beef Pho (Pho Bo)   \n",
       "6            7      7  Vietnamese Pho Recipe (Vietnamese Beef Noodle ...   \n",
       "7            8      8                  Pho {Vietnamese Beef Noodle Soup}   \n",
       "8            9      9                            Vietnamese Spring Rolls   \n",
       "9           10     10             Fresh and Easy Vietnamese Noodle Salad   \n",
       "10          11     11                      Vietnamese Lemongrass Chicken   \n",
       "11          12     12                        Easy Vietnamese Spring Roll   \n",
       "12          13     13                       Pho (Vietnamese Noodle Soup)   \n",
       "13          14     14                                Vietnamese Pho Soup   \n",
       "14          15     15                Shrimp Pho - Vietnamese Noodle Soup   \n",
       "15          16     16                         Grilled Vietnamese Chicken   \n",
       "16          17     17                      Vietnamese Fresh Spring Rolls   \n",
       "17          18     18                                     Vietnamese Pho   \n",
       "18          19     19                              Vietnamese Zoodle Pho   \n",
       "19          20     20                                 Vietnamese Pickles   \n",
       "\n",
       "                                          ingredients     cuisine  \n",
       "0   ['8 cups beef broth', '4 cups water', '1 yello...  Vietnamese  \n",
       "1   ['1 pound ground chicken', '3 tablespoons fish...  Vietnamese  \n",
       "2   ['1 pound pork blade steaks (boneless, about 1...  Vietnamese  \n",
       "3   ['1 pound chicken thighs (with skin, deboned)'...  Vietnamese  \n",
       "4   ['3 marrow bones (beef bone)', '50 ounces beef...  Vietnamese  \n",
       "5   ['3 pounds beef (knuckles or neck bones with m...  Vietnamese  \n",
       "6   ['600 grams onions (about 3 to 4 medium sized ...  Vietnamese  \n",
       "7   ['2 tablespoons coriander seeds (whole)', '1 c...  Vietnamese  \n",
       "8   ['12 spring rolls (wrappers)', '18 cooked shri...  Vietnamese  \n",
       "9   ['12 ounces vermicelli noodles (thin Asian, su...  Vietnamese  \n",
       "10  ['4 chicken thighs (boneless, skin on, Approxi...  Vietnamese  \n",
       "11  ['3/4 cup rice noodles (Vermicelli)', '1 packa...  Vietnamese  \n",
       "12  ['1 white onion (large, peeled and halved)', '...  Vietnamese  \n",
       "13  ['185 grams soup paste (jar AYAM™ Vietnamese P...  Vietnamese  \n",
       "14  ['8 ounces rice noodles (thin or pad thai styl...  Vietnamese  \n",
       "15  ['1 pound boneless skinless chicken thighs (or...  Vietnamese  \n",
       "16  ['2 ounces rice noodles (rice sticks)', '1/4 p...  Vietnamese  \n",
       "17  ['2 cups beef stock', '1 cinnamon (quill)', '2...  Vietnamese  \n",
       "18  ['1 sweet onion (large)', '2 inches fresh ging...  Vietnamese  \n",
       "19  ['1 pound carrots (about 3 medium carrots, pee...  Vietnamese  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df = pd.read_csv('training_data_w_ingredients.csv', sep=';', error_bad_lines=False)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients, list to string\n",
    "\n",
    "For every row we add a column called Ingredients_Str which contains the concatinated Strings from the Ingredients column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing \n",
    "Now that the data is retrieved, I need to process the data for the model to train on. I use the work from my local Jupyter notebook and make that work here in Sagemaker.\n",
    "\n",
    "## Writing a Custom Transformer \n",
    "When working locally on the data, I did not perform any predictions as I was using the data I had scraped, divided it into train/test to determine the model performance. Since I will be using the SKLearn pipeline for implementation and I plan to use it for predicting on new data (in the API endpoint), I need to format the parse_recipes function to use in a scikit-learn pipeline. It is not necessary but I would rather have it all in one pipeline.\n",
    "\n",
    "FunctionTransformer (simpler functions), Fixing for state\n",
    "\n",
    "A FunctionTransformer class helps to introduce arbitrary, stateless transforms into a Pipeline. Using a TransformerMixin class is overkill for what I need to do, though since I was able to get it to work with the recipe data, I am using it for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class TransformRecipe(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "    def transform(self,X, **kwargs):\n",
    "        recipe_list=X\n",
    "        ingr_list=[]\n",
    "        bigram_list=[]\n",
    "        recipe_string_list=[]\n",
    "\n",
    "#         ilist = [[word.lower() for word in i.split()] for i in ilist] \n",
    "        for recipe in recipe_list:\n",
    "#             print(recipe)\n",
    "            ingr_list=[]\n",
    "            for ingredient in recipe:\n",
    "\n",
    "                ingredient.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "                words = ingredient.split()\n",
    "                words = [''.join(c for c in word if c not in string.punctuation) for word in words]\n",
    "                words = [word for word in words if word.isalpha()]\n",
    "                words = [word.lower() for word in words] \n",
    "                words = [lemmatizer.lemmatize(word) for word in words]\n",
    "                words = [word for word in words if word not in measures]\n",
    "                words = [word for word in words if word not in common_remove]\n",
    "                words = [word for word in words if word not in data_leaks]\n",
    "                #get rid of any blank\n",
    "                words = list(filter(None, words))\n",
    "#                 print(\"before if length statements\")\n",
    "#                 print(words)\n",
    "                if(len(words)<=3):\n",
    "                    ingr_list.append(' '.join(words))\n",
    "\n",
    "                words = [word for word in words if word not in useless_singles]\n",
    "            #easiest way to deal with any duplicates or blanks for now\n",
    "                ingr_list = list(set(ingr_list))\n",
    "                #attempts to get rid of the blank\n",
    "#                 ingr_list = list(filter(None, ingr_list))\n",
    "#                 ingr_list = [x for x in ingr_list if x]   \n",
    "                if(len(words)>3): #handle rare case\n",
    "                    ingr_list.append(' '.join(words))\n",
    "                recipe_string=' '.join(ingr_list)\n",
    "#             print(recipe_string)\n",
    "\n",
    "            recipe_string_list.append(recipe_string)\n",
    "\n",
    "        return recipe_string_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words to Remove \n",
    "This code and these word lists are copied over as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/l/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Source for list below\n",
    "#https://en.wikipedia.org/wiki/Cooking_weights_and_measures\n",
    "#https://thebakingpan.com/ingredient-weights-and-measures/\n",
    "measures=['litrbes','liter','millilitres','mL','grams','g', 'kg','teaspoon','tsp', 'tablespoon','tbsp','fluid', 'ounce','oz','fl.oz', 'cup','pint','pt','quart','qt','gallon','gal','smidgen','drop','pinch','dash','scruple','dessertspoon','teacup','cup','c','pottle','gill','dram','wineglass','coffeespoon','pound','pounded','lb','tbsp','plus','firmly', 'packed','lightly','level','even','rounded','heaping','heaped','sifted','bushel','peck','stick','chopped','sliced','halves', 'shredded','slivered','sliced','whole','paste','whole',' fresh', 'peeled', 'diced','mashed','dried','frozen','fresh','peeled','candied','no', 'pulp','crystallized','canned','crushed','minced','julienned','clove','head', 'small','large','medium', 'torn', 'cleaned', 'degree']\n",
    "\n",
    "measures = [lemmatizer.lemmatize(m) for m in measures]\n",
    "#some of these include data leakage words, like 'italian' - ok to remove after including bigrams\n",
    "data_leaks = ['iberic','greek', 'korean','italianstyle', 'french','thai', 'chinese', 'mexican','spanish','indian','italian']\n",
    "\n",
    "common_remove=['ground','to','taste', 'and', 'or',  'can',  'into', 'cut', 'grated', 'leaf','package','finely','divided','a','piece','optional','inch','needed','more','drained','for','flake','dry','thinly','cubed','bunch','cube','slice','pod','beaten','seeded','uncooked','root','plain','heavy','halved','crumbled','sweet','with','hot','room','temperature','trimmed','allpurpose','deveined','bulk','seasoning','jar','food','if','bag','mix','in','each','roll','instant','double','such','frying','thawed','whipping','stock','rinsed','mild','sprig','freshly','toasted','link','boiling','cooked','unsalted','container',\n",
    "'cooking','thin','lengthwise','warm','softened','thick','quartered','juiced','pitted','chunk','melted','cold','coloring','puree','cored','stewed','floret','coarsely','the','blanched','zested','sweetened','powdered','garnish','dressing','soup','at','active','lean','chip','sour','long','ripe','skinned','fillet','from','stem','flaked','removed','stalk','unsweetened','cover','crust', 'extra', 'prepared', 'blend', 'of', 'ring',  'undrained', 'about', 'zest', ' ', '', 'spray', 'round', 'herb', 'seasoned', 'wedge', 'bitesize', 'broken', 'square', 'freshly', 'thickly', 'diagonally']\n",
    "common_remove = [lemmatizer.lemmatize(c) for c in common_remove]\n",
    "data_leaks = [lemmatizer.lemmatize(d) for d in data_leaks]\n",
    "# due to using bigrams not including \n",
    "useless_singles=['','black','white','red','yellow','seed','breast','confectioner','sundried','broth','bell','baby','juice','crumb','sauce','condensed','smoked','basmati','extravirgin','brown','clarified', 'soy', 'filling', 'pine', 'virgin', 'romano', 'heart', 'shell', 'thigh', 'boneless','skinless','split', 'dark', 'wheat', 'light', 'green', 'vegetable', 'curry', 'orange', 'garam', 'sesame', 'strip', 'sea', 'canola', 'mustard','powder', 'ice', 'bay', 'roasted', 'loaf', 'roast', 'powder']\n",
    "useless_singles = [lemmatizer.lemmatize(u) for u in useless_singles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create SKLearn Pipeline \n",
    "Here I put together the pipeline, using the class weighted Logistic Regression model. Additionally, the TransformRecipe class I wrote above is used here as part of the pipeline. It was easier for me to work with the data as a list of lists instead of a pandas dataframe, I can use a list or list of lists as input to the API, and since it works, I won't change it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "results for LR count vector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\", line 307, in _fit\n",
      "    **fit_params_steps[name])\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1204, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1134, in _count_vocab\n",
      "    raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
      "ValueError: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\", line 307, in _fit\n",
      "    **fit_params_steps[name])\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1204, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1134, in _count_vocab\n",
      "    raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
      "ValueError: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "  FitFailedWarning)\n",
      "/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:614: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\", line 307, in _fit\n",
      "    **fit_params_steps[name])\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/joblib/memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1204, in fit_transform\n",
      "    self.fixed_vocabulary_)\n",
      "  File \"/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\", line 1134, in _count_vocab\n",
      "    raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
      "ValueError: empty vocabulary; perhaps the documents only contain stop words\n",
      "\n",
      "  FitFailedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'fit_time': array([0.94669032, 0.91727138, 0.93845272]),\n",
      "    'score_time': array([0., 0., 0.]),\n",
      "    'test_score': array([nan, nan, nan]),\n",
      "    'train_score': array([nan, nan, nan])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-42b22edaafc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ingredients'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cuisine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cuisine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mlr_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# #Pickle pipeline after caling the fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[0;32m/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1204\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/l/anaconda3/envs/datascience/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1132\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1134\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m   1135\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m   1136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate,StratifiedKFold, train_test_split\n",
    "import pprint\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "skf=StratifiedKFold(n_splits=3)\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "\n",
    "print(\"\\nresults for LR count vector\")\n",
    "lr_pipe = Pipeline([('parse_recipe_text', TransformRecipe()),\n",
    "    ('vect', CountVectorizer(ngram_range=(1, 2))), \n",
    "    ('lr', LogisticRegression( max_iter=1000,random_state=123, \n",
    "    class_weight='balanced',multi_class='multinomial', solver='lbfgs'))])\n",
    "\n",
    "\n",
    "cv=cross_validate(lr_pipe, df['ingredients'], df['cuisine'].values, scoring='f1_weighted', \n",
    "                         cv=skf, return_train_score=True )\n",
    "\n",
    "pp.pprint(cv)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['ingredients'].values, df['cuisine'], test_size=0.25, stratify=df['cuisine'].tolist())\n",
    "\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "# #Pickle pipeline after caling the fit\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(lr_pipe, 'lr_pipe.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
